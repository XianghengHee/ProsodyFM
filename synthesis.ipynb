{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5876c0-b47e-4c80-9e9c-62550f81b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "# Hifigan imports\n",
    "from prosodyfm.hifigan.config import v1\n",
    "from prosodyfm.hifigan.denoiser import Denoiser\n",
    "from prosodyfm.hifigan.env import AttrDict\n",
    "from prosodyfm.hifigan.models import Generator as HiFiGAN\n",
    "\n",
    "from prosodyfm.models.prosodyfm import ProsodyFM\n",
    "from prosodyfm.text import sequence_to_text, text_to_sequence\n",
    "from prosodyfm.utils.model import denormalize\n",
    "from prosodyfm.utils.utils import get_user_data_dir, intersperse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a30306-588c-4f22-8d9b-e2676880b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "# This allows for real time code changes being reflected in the notebook, no need to restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a312856b-01a9-4d75-a4c8-4666dffa0692",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7640a4c1-44ce-447c-a8ff-45012fb7bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROSODYFM_CHECKPOINT = './checkpoints/checkpoint_epoch=349.ckpt'\n",
    "HIFIGAN_CHECKPOINT = './hifigan/released_checkpoints/g_02500000'\n",
    "OUTPUT_FOLDER = \"./demo_samples/with_boundary_gst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a16230-04ba-4825-a844-2fb5ab945e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path):\n",
    "    model = ProsodyFM.load_from_checkpoint(checkpoint_path, map_location=device)\n",
    "    model.eval()\n",
    "    return model\n",
    "count_params = lambda x: f\"{sum(p.numel() for p in x.parameters()):,}\"\n",
    "\n",
    "\n",
    "model = load_model(PROSODYFM_CHECKPOINT)\n",
    "print(f\"Model loaded! Parameter count: {count_params(model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3077b84b-e3b6-42e1-a84b-2f7084b13f92",
   "metadata": {},
   "source": [
    "## Load HiFi-GAN (Vocoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b68184-968d-4868-9029-f0c40e9e68af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocoder(checkpoint_path):\n",
    "    h = AttrDict(v1)\n",
    "    hifigan = HiFiGAN(h).to(device)\n",
    "    hifigan.load_state_dict(torch.load(checkpoint_path, map_location=device)['generator'])\n",
    "    _ = hifigan.eval()\n",
    "    hifigan.remove_weight_norm()\n",
    "    return hifigan\n",
    "\n",
    "vocoder = load_vocoder(HIFIGAN_CHECKPOINT)\n",
    "denoiser = Denoiser(vocoder, mode='zeros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de082e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./libritts_audio_sid_text_test_filelist_b_pitchseg_t5_final.pkl\", \"rb\") as f:\n",
    "    t5_filelist = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96c0a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testing_data(test_filelist, index):\n",
    "    info = test_filelist[index]\n",
    "    wav_file = info[0]\n",
    "    text = info[2]\n",
    "    sequence = text_to_sequence(text, ['english_cleaners2'])\n",
    "    x = torch.tensor(intersperse(sequence, 0),dtype=torch.long, device=device)[None]\n",
    "    x_lengths = torch.tensor([x.shape[-1]],dtype=torch.long, device=device)\n",
    "    x_phones = sequence_to_text(x.squeeze(0).tolist())\n",
    "    \n",
    "    spk = info[1]\n",
    "    spk = torch.tensor([int(spk)], device=device, dtype=torch.long)\n",
    "    \n",
    "    #x_pitch_seg = custom_pitch\n",
    "    x_pitch_seg = info[3]\n",
    "    x_pitch_seg = torch.from_numpy(x_pitch_seg).float().to(device)\n",
    "\n",
    "    x_pitch_seg_lengths = torch.from_numpy(info[4]).long().to(device)\n",
    "    x_last_word_num = torch.tensor([info[5]], device=device, dtype=torch.long)\n",
    "        \n",
    "    return {\n",
    "    'x': x,\n",
    "    'x_orig': text,\n",
    "    'x_lengths': x_lengths,\n",
    "    'spk': spk,\n",
    "    'x_pitch_seg': x_pitch_seg,\n",
    "    'x_pitch_seg_lengths': x_pitch_seg_lengths,\n",
    "    'x_last_word_num': x_last_word_num,\n",
    "    'x_phones': x_phones,\n",
    "    'wav_file': wav_file\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880a1879-24fd-4757-849c-850339120796",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def synthesise(test_filelist, index):\n",
    "    processed_data = get_testing_data(test_filelist, index)\n",
    "    start_t = dt.datetime.now()\n",
    "    output = model.synthesise(\n",
    "        x = processed_data['x'], \n",
    "        x_lengths = processed_data['x_lengths'],\n",
    "        n_timesteps=10,\n",
    "        spks=processed_data['spk'],\n",
    "        x_pitch_seg = processed_data['x_pitch_seg'],\n",
    "        x_pitch_seg_lengths = processed_data['x_pitch_seg_lengths'],\n",
    "        x_last_word_num = processed_data['x_last_word_num'],\n",
    "        length_scale=1.0,\n",
    "        temperature=0.667,\n",
    "    )\n",
    "    \n",
    "\n",
    "    # merge everything to one dict    \n",
    "    output.update({'start_t': start_t, **processed_data})\n",
    "    return output\n",
    "\n",
    "@torch.inference_mode()\n",
    "def to_waveform(mel, vocoder):\n",
    "    audio = vocoder(mel).clamp(-1, 1)\n",
    "    audio = denoiser(audio.squeeze(0), strength=0.00025).cpu().squeeze()\n",
    "    return audio.cpu().squeeze()\n",
    "    \n",
    "def save_to_folder(filename: str, output: dict, folder: str):\n",
    "    folder = Path(folder)\n",
    "    folder.mkdir(exist_ok=True, parents=True)\n",
    "    #np.save(folder / f'{filename}', output['mel'].cpu().numpy())\n",
    "    sf.write(folder / f'{filename}', output['waveform'], 22050, 'PCM_24')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f857e3-2ef7-4c86-b776-596c4d3cf875",
   "metadata": {},
   "source": [
    "## Setup text to synthesise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9da9e2d-99b9-4c6f-8a08-c828e2cba121",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d216e5-4895-4da8-9d24-9e61021d2556",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of ODE Solver steps\n",
    "n_timesteps = 10\n",
    "\n",
    "## Changes to the speaking rate\n",
    "length_scale=1.0\n",
    "\n",
    "## Sampling temperature\n",
    "temperature = 0.667"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93aac89-c7f8-4975-8510-4e763c9689f4",
   "metadata": {},
   "source": [
    "## Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a227963-aa12-43b9-a706-1168b6fc0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, rtfs = [], []\n",
    "rtfs_w = []\n",
    "\n",
    "for i in range(len(t5_filelist)):\n",
    "    output = synthesise(t5_filelist, i) #, torch.tensor([15], device=device, dtype=torch.long).unsqueeze(0))\n",
    "    output['waveform'] = to_waveform(output['mel'], vocoder)\n",
    "\n",
    "    # Compute Real Time Factor (RTF) with HiFi-GAN\n",
    "    t = (dt.datetime.now() - output['start_t']).total_seconds()\n",
    "    rtf_w = t * 22050 / (output['waveform'].shape[-1])\n",
    "\n",
    "    ## Pretty print\n",
    "    print(f\"{'*' * 53}\")\n",
    "    print(f\"Input text - {i}\")\n",
    "    print(f\"{'-' * 53}\")\n",
    "    print(output['x_orig'])\n",
    "    print(f\"{'*' * 53}\")\n",
    "    print(f\"Phonetised text - {i}\")\n",
    "    print(f\"{'-' * 53}\")\n",
    "    print(output['x_phones'])\n",
    "    print(f\"{'*' * 53}\")\n",
    "    print(f\"Speaker Id - {output['spk']}\")\n",
    "    print(f\"{'-' * 53}\")\n",
    "    print(output['wav_file'])\n",
    "    print(f\"{'*' * 53}\")\n",
    "    print(f\"RTF:\\t\\t{output['rtf']:.6f}\")\n",
    "    print(f\"RTF Waveform:\\t{rtf_w:.6f}\")\n",
    "    rtfs.append(output['rtf'])\n",
    "    rtfs_w.append(rtf_w)\n",
    "\n",
    "    ## Display the synthesised waveform\n",
    "    ipd.display(ipd.Audio(output['waveform'], rate=22050))\n",
    "    wav_name = output['wav_file'].split('/')[-1]\n",
    "    ## Save the generated waveform\n",
    "    save_to_folder(wav_name, output, OUTPUT_FOLDER)\n",
    "\n",
    "print(f\"Number of ODE steps: {n_timesteps}\")\n",
    "print(f\"Mean RTF:\\t\\t\\t\\t{np.mean(rtfs):.6f} ± {np.std(rtfs):.6f}\")\n",
    "print(f\"Mean RTF Waveform (incl. vocoder):\\t{np.mean(rtfs_w):.6f} ± {np.std(rtfs_w):.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prosodyfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
